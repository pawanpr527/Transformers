# -*- coding: utf-8 -*-
"""bert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A5P9zrHpjgjeHwtHztBzgxMhuOyIDeaf
"""

# Transformers Module :

#Model Classes :
1.BertModel
2.BertForMaskedLM
3.BertForSequenceClassification
4.BertForQuestionAnswering
5.GPTLMHeadModel
6.T5ForConditionalGeneration


#Tokenizer Class :
1.BertTokenizer
2.GPT2Tokenizer
3.T5Tokenizer
4.AutoTokenizer

#Config Class :
1.BertConfig
2.BertModel

#Training Utilites (fine tuning) :
1.Trainer
2.TrainingArguments

#Model Saving
1.model.save_pretrained()
2.tokenizer.save_pretrained()


# Custom Model

from transformers import BertConfig, BertModel

# Load default config for bert-base-uncased
config = BertConfig.from_pretrained("bert-base-uncased")
print(config)
custom_config = BertConfig(
    hidden_size=512,         # smaller hidden dimension
    num_hidden_layers=6,     # fewer transformer layers
    num_attention_heads=8,   # fewer attention heads
    vocab_size=40000         # custom tokenizer vocab size
)

# Create a new model with this config (random weights, not pretrained!)
model = BertModel(custom_config)

