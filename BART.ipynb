{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEbIp-BVojat"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as f\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self,d_model,max_length=5000):\n",
        "     super().__init__()\n",
        "     pe = torch.zeros(max_length,d_model)\n",
        "     position = torch.arange(0,max_length,dtype=torch.float).unsqueeze(1)\n",
        "     div_term = torch.exp(torch.arange(0,d_model,2).float() * (-math.log(1000.0)/d_model))\n",
        "     pe[:,0::2] = torch.sin(position * div_term)\n",
        "     pe[:,1::2] = torch.cos(position * div_term)\n",
        "     pe = pe.unsqueeze(0)\n",
        "     self.register_buffer('pe',pe)\n",
        "  def forward(self,x):\n",
        "    seq_len = x.size(1)\n",
        "    return x + self.pe[:,seq_len].to(x.device)\n",
        "\n"
      ],
      "metadata": {
        "id": "ub63ta-Boqlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "   def __init__(self, dim, eps= 1e-5):\n",
        "      super().__init__()\n",
        "      self.ln = nn.LayerNorm(dim,eps=eps)\n",
        "   def forward(self,x):\n",
        "      return self.ln(x)"
      ],
      "metadata": {
        "id": "lpOnGLmvr58r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "   def __init__(self,d_model,d_ff,dropout=0.1):\n",
        "      super().__init__()\n",
        "      self.fc1 = nn.Linear(d_model,d_ff)\n",
        "      self.fc2 = nn.Linear(d_ff,d_model)\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "      self.activation = f.gelu\n",
        "   def forwad(self,x):\n",
        "     x = self.fc1(x)\n",
        "     x = self.activation(x)\n",
        "     x = self.dropout(x)\n",
        "     x = self.fc2(x)\n",
        "     return x"
      ],
      "metadata": {
        "id": "UmC1S3PXuKBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#.  Attention Module\n",
        "class scaled_dot_product_attention(nn.Module):\n",
        "  def __init__(self,q,k,v,mask=None,dropout=None):\n",
        "     super().__init__()\n",
        "     dk = q.size(-1)\n",
        "     scores = torch.matul(q,k.transpose(-2,-1))/math.sqrt(dk)\n",
        "     if mask is not None:\n",
        "       scores = scores.mask_fill(mask==0,float=('-1e9'))\n",
        "     attn = torch.softmax(scores,dim=-1)\n",
        "     if dropout is not None:\n",
        "        attn = dropout(attn)\n",
        "     output = torch.matmul(attn,v)\n",
        "     return output , attn\n",
        ""
      ],
      "metadata": {
        "id": "z_wXHjmE04IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "   def __init__(self,d_model,n_heads,dropout=0.1):\n",
        "     super().__init__()\n",
        "     assert d_model % n_heads==0\n",
        "     self.d_model = d_model\n",
        "     self.n_heads = n_heads\n",
        "     self.d_heads  = d_model // n_heads\n",
        "\n",
        "     self.w_q = nn.Linear(d_model,d_model)\n",
        "     self.w_k = nn.Linear(d_model,d_model)\n",
        "     self.w_v = nn.Linear(d_model,d_model)\n",
        "     self.w_o = nn.Linear(d_model,d_model)\n",
        "\n",
        "     self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "   def forwad(self,query,key,value,mask=None):\n",
        "      B = query.size(0)\n",
        "\n",
        "      q = self.w_q(query).view(B,-1,self.n_heads,self.n_heads).transpose(1,2)\n",
        "      k = self.w_k(key).view(B, -1, self.n_heads, self.d_head).transpose(1, 2) # (B, n_heads, seq_k, d_head)\n",
        "      v = self.w_v(value).view(B, -1, self.n_heads, self.d_head).transpose(1, 2) # (B, n_heads, seq_v, d_head)\n",
        "      if mask is not None:\n",
        "        if mask.dim() == 3:\n",
        "             mask = mask.unsqueeze(1) # add head dim so it can broadcast\n",
        "      attn_output, attn_weights = scaled_dot_product_attention(q, k, v, mask=mask, dropout=self.dropout)\n",
        "      attn_output = attn_output.transpose(1, 2).contiguous().view(B, -1, self.d_model) # (B, seq_q, d_model)\n",
        "      output = self.w_o(attn_output) # linear projection to d_model\n",
        "      return output, attn_weights\n",
        "\n"
      ],
      "metadata": {
        "id": "UDzaUA6_x6dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "     def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout=dropout) # self-attention\n",
        "        self.layernorm1 = LayerNorm(d_model) # layernorm after residual 1\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout=dropout) # feed-forward network\n",
        "        self.layernorm2 = LayerNorm(d_model) # layernorm after residual 2\n",
        "        self.dropout = nn.Dropout(dropout) # dropout for residuals\n",
        "\n",
        "\n",
        "     def forward(self, x, src_mask=None):\n",
        "# x shape: (B, seq_len, d_model)\n",
        "       attn_out, _ = self.self_attn(x, x, x, mask=src_mask) # self-attention where Q=K=V=x\n",
        "       x = x + self.dropout(attn_out) # residual connection\n",
        "       x = self.layernorm1(x) # layer normalization\n",
        "       ff_out = self.ff(x) # feed-forward\n",
        "       x = x + self.dropout(ff_out) # residual connection\n",
        "       x = self.layernorm2(x) # layer normalization\n",
        "       return x"
      ],
      "metadata": {
        "id": "k9Rkf91-x50s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "     def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "          super().__init__()\n",
        "          self.self_attn = MultiHeadAttention(d_model, n_heads, dropout=dropout) # masked self-attention\n",
        "          self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout=dropout) # cross-attention over encoder outputs\n",
        "          self.layernorm1 = LayerNorm(d_model) # norm after self-attn residual\n",
        "          self.layernorm2 = LayerNorm(d_model) # norm after cross-attn residual\n",
        "          self.ff = FeedForward(d_model, d_ff, dropout=dropout) # feed-forward\n",
        "          self.layernorm3 = LayerNorm(d_model) # norm after ff residual\n",
        "          self.dropout = nn.Dropout(dropout)\n",
        "     def forward(self, x, enc_out, tgt_mask=None, memory_mask=None):\n",
        "# x: (B, tgt_seq, d_model), enc_out: (B, src_seq, d_model)\n",
        "         self_attn_out, _ = self.self_attn(x, x, x, mask=tgt_mask) # masked self-attention\n",
        "         x = x + self.dropout(self_attn_out) # residual\n",
        "         x = self.layernorm1(x) # norm\n",
        "\n",
        "\n",
        "         cross_attn_out, _ = self.cross_attn(x, enc_out, enc_out, mask=memory_mask) # cross-attention\n",
        "         x = x + self.dropout(cross_attn_out) # residual\n",
        "         x = self.layernorm2(x) # norm\n",
        "\n",
        "\n",
        "         ff_out = self.ff(x) # feed-forward\n",
        "         x = x + self.dropout(ff_out) # residual\n",
        "         x = self.layernorm3(x) # norm\n",
        "         return x\n",
        ""
      ],
      "metadata": {
        "id": "HtuK8Ctj1z-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------- Full Encoder & Decoder stacks --------------------------\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=512, n_heads=8, num_layers=6, d_ff=2048, max_len=1024, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)  # token embeddings\n",
        "        self.pos_embedding = PositionalEncoding(d_model, max_len=max_len)  # positional encodings\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)\n",
        "        ])  # stack of encoder layers\n",
        "        self.dropout = nn.Dropout(dropout)  # dropout on embeddings\n",
        "\n",
        "    def forward(self, src_tokens, src_mask=None):\n",
        "        # src_tokens shape: (B, src_seq)\n",
        "        x = self.token_embedding(src_tokens)      # (B, src_seq, d_model)\n",
        "        x = self.pos_embedding(x)                 # add positional encodings\n",
        "        x = self.dropout(x)                       # apply dropout\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_mask=src_mask)       # pass through encoder layers\n",
        "        return x                                  # encoder memory\n",
        "\n",
        "\n",
        "# Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=512, n_heads=8, num_layers=6, d_ff=2048, max_len=1024, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)  # token embeddings for decoder\n",
        "        self.pos_embedding = PositionalEncoding(d_model, max_len=max_len)  # positional encodings\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)\n",
        "        ])  # decoder layers\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt_tokens, enc_out, tgt_mask=None, memory_mask=None):\n",
        "        # tgt_tokens: (B, tgt_seq)\n",
        "        x = self.token_embedding(tgt_tokens)                           # embed target tokens\n",
        "        x = self.pos_embedding(x)                                      # add positional info\n",
        "        x = self.dropout(x)                                            # apply dropout\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out, tgt_mask=tgt_mask, memory_mask=memory_mask)  # decoder layers\n",
        "        return x                                                       # decoder outputs"
      ],
      "metadata": {
        "id": "1676KYFi2aHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleBART(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=512, n_heads=8,\n",
        "                 num_encoder_layers=6, num_decoder_layers=6,\n",
        "                 d_ff=2048, max_len=1024, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # Encoder and Decoder stacks\n",
        "        self.encoder = Encoder(vocab_size, d_model, n_heads,\n",
        "                               num_encoder_layers, d_ff, max_len, dropout)\n",
        "        self.decoder = Decoder(vocab_size, d_model, n_heads,\n",
        "                               num_decoder_layers, d_ff, max_len, dropout)\n",
        "        # Final LM head: maps decoder outputs → vocabulary logits\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "        # Initialize parameters\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        \"\"\"Initialize parameters with Xavier uniform (like Transformer defaults).\"\"\"\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def _make_src_mask(self, src_tokens, pad_token_id=1):\n",
        "        \"\"\"\n",
        "        Create encoder attention mask.\n",
        "        src_tokens: (B, src_seq)\n",
        "        Returns: mask (B, 1, 1, src_seq) with True for non-pad tokens\n",
        "        \"\"\"\n",
        "        src_mask = (src_tokens != pad_token_id).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask\n",
        "\n",
        "    def _make_tgt_mask(self, tgt_tokens, pad_token_id=1):\n",
        "        \"\"\"\n",
        "        Create decoder attention mask.\n",
        "        Combines:\n",
        "        - padding mask\n",
        "        - subsequent mask (causal masking: prevents attending to future tokens)\n",
        "        \"\"\"\n",
        "        # Padding mask: (B, 1, 1, tgt_seq)\n",
        "        tgt_pad_mask = (tgt_tokens != pad_token_id).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Subsequent mask: lower triangular (tgt_seq, tgt_seq)\n",
        "        seq_len = tgt_tokens.size(1)\n",
        "        subsequent_mask = torch.tril(\n",
        "            torch.ones((seq_len, seq_len), device=tgt_tokens.device)\n",
        "        ).bool()\n",
        "        subsequent_mask = subsequent_mask.unsqueeze(0).unsqueeze(1)  # (1, 1, seq, seq)\n",
        "\n",
        "        # Combine both\n",
        "        tgt_mask = tgt_pad_mask & subsequent_mask\n",
        "        return tgt_mask\n",
        "\n",
        "    def forward(self, src_tokens, tgt_tokens, src_pad_token_id=1, tgt_pad_token_id=1):\n",
        "        \"\"\"\n",
        "        Forward pass through SimpleBART.\n",
        "        src_tokens: (B, src_seq)\n",
        "        tgt_tokens: (B, tgt_seq)\n",
        "        Returns: logits (B, tgt_seq, vocab_size)\n",
        "        \"\"\"\n",
        "        # Masks\n",
        "        src_mask = self._make_src_mask(src_tokens, pad_token_id=src_pad_token_id)\n",
        "        tgt_mask = self._make_tgt_mask(tgt_tokens, pad_token_id=tgt_pad_token_id)\n",
        "        memory_mask = src_mask  # cross-attention uses src mask\n",
        "\n",
        "        # Encoder\n",
        "        enc_out = self.encoder(src_tokens, src_mask=src_mask)\n",
        "\n",
        "        # Decoder\n",
        "        dec_out = self.decoder(tgt_tokens, enc_out,\n",
        "                               tgt_mask=tgt_mask, memory_mask=memory_mask)\n",
        "\n",
        "        # Final linear layer → vocab logits\n",
        "        logits = self.lm_head(dec_out)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "wKCCaON33nzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, src_tokens, src_pad_token_id=1,\n",
        "                  max_len=50, start_token_id=2, end_token_id=3):\n",
        "    \"\"\"\n",
        "    Simple greedy decoding loop.\n",
        "\n",
        "    Args:\n",
        "        model: SimpleBART model\n",
        "        src_tokens: (B, src_seq) input source token IDs\n",
        "        src_pad_token_id: pad token ID for source\n",
        "        max_len: maximum length of generated sequence\n",
        "        start_token_id: ID for <BOS> / start token\n",
        "        end_token_id: ID for <EOS> / end token\n",
        "\n",
        "    Returns:\n",
        "        ys: (B, generated_seq_len) tensor of generated token IDs\n",
        "    \"\"\"\n",
        "    model.eval()  # evaluation mode (disable dropout etc.)\n",
        "    B = src_tokens.size(0)   # batch size\n",
        "    device = src_tokens.device\n",
        "\n",
        "    # Create encoder mask and encode source once\n",
        "    src_mask = model._make_src_mask(src_tokens, pad_token_id=src_pad_token_id)\n",
        "    with torch.no_grad():\n",
        "        enc_out = model.encoder(src_tokens, src_mask=src_mask)\n",
        "\n",
        "        # Initialize decoder input with <BOS> token\n",
        "        ys = torch.full((B, 1), start_token_id, dtype=torch.long, device=device)\n",
        "\n",
        "        # Loop to generate tokens\n",
        "        for _ in range(max_len - 1):\n",
        "            # Create target mask for current sequence\n",
        "            tgt_mask = model._make_tgt_mask(ys, pad_token_id=0)\n",
        "\n",
        "            # Decoder forward pass\n",
        "            dec_out = model.decoder(ys, enc_out,\n",
        "                                    tgt_mask=tgt_mask,\n",
        "                                    memory_mask=src_mask)\n",
        "\n",
        "            # Project to vocabulary logits\n",
        "            logits = model.lm_head(dec_out)  # (B, seq_len, vocab_size)\n",
        "\n",
        "            # Pick the most probable next token (greedy)\n",
        "            next_tok = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
        "\n",
        "            # Append to sequence\n",
        "            ys = torch.cat([ys, next_tok], dim=1)\n",
        "\n",
        "            # Stop if all sequences ended with <EOS>\n",
        "            if (next_tok == end_token_id).all():\n",
        "                break\n",
        "\n",
        "    return ys  # (B, generated_seq_len)\n"
      ],
      "metadata": {
        "id": "e-8gnHnm39nk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}