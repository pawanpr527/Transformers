{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzU9C_jMg8wJ",
        "outputId": "0bb3e450-74f4-4c44-a99f-cbb6bede163a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create tensor\n",
        "x = torch.tensor([1,2,4])\n",
        "print(x)\n",
        "#move tensor to cuda or gpu\n",
        "x = x.to(device)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MLpv3dCkzYr",
        "outputId": "1537930c-3bae-4eb5-e614-b1eebf9827f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 4])\n",
            "tensor([1, 2, 4], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Simple character-level tokenizer\n",
        "text = \"hello world\"\n",
        "chars = sorted(list(set(text)))\n",
        "print(chars)\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "print(stoi)\n",
        "itos = {i:ch for ch,i in stoi.items()}\n",
        "print(itos)\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"hello\"))\n",
        "print(decode([3,2,4,4,5]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlPngtFplL1s",
        "outputId": "f389934d-1eed-49e8-906c-3da4f7b63f8a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' ', 'd', 'e', 'h', 'l', 'o', 'r', 'w']\n",
            "{' ': 0, 'd': 1, 'e': 2, 'h': 3, 'l': 4, 'o': 5, 'r': 6, 'w': 7}\n",
            "{0: ' ', 1: 'd', 2: 'e', 3: 'h', 4: 'l', 5: 'o', 6: 'r', 7: 'w'}\n",
            "[3, 2, 4, 4, 5]\n",
            "hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(stoi)\n",
        "embed_dim = 32\n",
        "embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "x = torch.tensor([encode(\"hello\")])\n",
        "emb = embedding(x)\n",
        "print(emb.shape)  # (1,5,32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCajnGD5pBrF",
        "outputId": "71d38460-4299-44b3-9773-28e429167a52"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, heads=1):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.fc = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        print(f\"Input shape: {x.shape}\")\n",
        "        print(f\"Input x:\\n{x}\\n\")\n",
        "\n",
        "        # Linear projection to Q, K, V\n",
        "        qkv = self.qkv(x)\n",
        "        print(f\"After qkv projection shape: {qkv.shape}\")\n",
        "        print(f\"qkv projection:\\n{qkv}\\n\")\n",
        "\n",
        "        # Split into Q, K, V\n",
        "        qkv = qkv.chunk(3, dim=-1)\n",
        "        Q, K, V = qkv\n",
        "        print(f\"Q shape: {Q.shape}\\nQ:\\n{Q}\\n\")\n",
        "        print(f\"K shape: {K.shape}\\nK:\\n{K}\\n\")\n",
        "        print(f\"V shape: {V.shape}\\nV:\\n{V}\\n\")\n",
        "\n",
        "        # Attention scores\n",
        "        attn_scores = (Q @ K.transpose(-2, -1)) / (C**0.5)\n",
        "        print(f\"Attention scores shape: {attn_scores.shape}\")\n",
        "        print(f\"Attention scores:\\n{attn_scores}\\n\")\n",
        "\n",
        "        # Softmax to get attention weights\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        print(f\"Attention weights shape: {attn_weights.shape}\")\n",
        "        print(f\"Attention weights:\\n{attn_weights}\\n\")\n",
        "\n",
        "        # Weighted sum of values\n",
        "        out = attn_weights @ V\n",
        "        print(f\"Output before final linear layer shape: {out.shape}\")\n",
        "        print(f\"Output before fc:\\n{out}\\n\")\n",
        "\n",
        "        # Final projection\n",
        "        final_output = self.fc(out)\n",
        "        print(f\"Final output shape: {final_output.shape}\")\n",
        "        print(f\"Final output:\\n{final_output}\\n\")\n",
        "\n",
        "        return final_output\n",
        "\n",
        "# ---- Sample input ----\n",
        "torch.manual_seed(0)  # for reproducibility\n",
        "\n",
        "# Batch size = 1, Sequence length = 3, Embedding size = 4\n",
        "x = torch.randn(1, 3, 4)\n",
        "\n",
        "# Initialize and run the attention module\n",
        "attn = SelfAttention(embed_dim=4)\n",
        "out = attn(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMM2YcwDr7s6",
        "outputId": "e3341ba7-e4d6-47a9-9e52-27ba2f7cf1ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([1, 3, 4])\n",
            "Input x:\n",
            "tensor([[[ 1.5410, -0.2934, -2.1788,  0.5684],\n",
            "         [-1.0845, -1.3986,  0.4033,  0.8380],\n",
            "         [-0.7193, -0.4033, -0.5966,  0.1820]]])\n",
            "\n",
            "After qkv projection shape: torch.Size([1, 3, 12])\n",
            "qkv projection:\n",
            "tensor([[[-1.1933,  0.6366,  0.4981, -0.4831, -0.2344, -1.4366, -0.7046,\n",
            "          -0.9246,  0.3996,  1.4650,  0.8459,  0.5737],\n",
            "         [ 0.0038,  1.2161,  0.5752,  0.4497,  0.0724,  0.6905,  0.8573,\n",
            "          -0.7340, -0.3811, -0.0752,  0.2755, -1.8156],\n",
            "         [-0.1214,  0.8867,  0.8842,  0.3888,  0.0485, -0.2058,  0.1125,\n",
            "          -0.7364, -0.2169,  0.4046,  0.2179, -0.7190]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "\n",
            "Q shape: torch.Size([1, 3, 4])\n",
            "Q:\n",
            "tensor([[[-1.1933,  0.6366,  0.4981, -0.4831],\n",
            "         [ 0.0038,  1.2161,  0.5752,  0.4497],\n",
            "         [-0.1214,  0.8867,  0.8842,  0.3888]]], grad_fn=<SplitBackward0>)\n",
            "\n",
            "K shape: torch.Size([1, 3, 4])\n",
            "K:\n",
            "tensor([[[-0.2344, -1.4366, -0.7046, -0.9246],\n",
            "         [ 0.0724,  0.6905,  0.8573, -0.7340],\n",
            "         [ 0.0485, -0.2058,  0.1125, -0.7364]]], grad_fn=<SplitBackward0>)\n",
            "\n",
            "V shape: torch.Size([1, 3, 4])\n",
            "V:\n",
            "tensor([[[ 0.3996,  1.4650,  0.8459,  0.5737],\n",
            "         [-0.3811, -0.0752,  0.2755, -1.8156],\n",
            "         [-0.2169,  0.4046,  0.2179, -0.7190]]], grad_fn=<SplitBackward0>)\n",
            "\n",
            "Attention scores shape: torch.Size([1, 3, 3])\n",
            "Attention scores:\n",
            "tensor([[[-0.2695,  0.5674,  0.1114],\n",
            "         [-1.2845,  0.5015, -0.2583],\n",
            "         [-1.1139,  0.5380, -0.1876]]], grad_fn=<DivBackward0>)\n",
            "\n",
            "Attention weights shape: torch.Size([1, 3, 3])\n",
            "Attention weights:\n",
            "tensor([[[0.2095, 0.4838, 0.3067],\n",
            "         [0.1025, 0.6115, 0.2860],\n",
            "         [0.1144, 0.5968, 0.2888]]], grad_fn=<SoftmaxBackward0>)\n",
            "\n",
            "Output before final linear layer shape: torch.Size([1, 3, 4])\n",
            "Output before fc:\n",
            "tensor([[[-0.1672,  0.3946,  0.3774, -0.9787],\n",
            "         [-0.2541,  0.2199,  0.3175, -1.2570],\n",
            "         [-0.2444,  0.2396,  0.3241, -1.2255]]], grad_fn=<UnsafeViewBackward0>)\n",
            "\n",
            "Final output shape: torch.Size([1, 3, 4])\n",
            "Final output:\n",
            "tensor([[[ 0.6144,  0.0096, -0.0734,  0.0083],\n",
            "         [ 0.6896, -0.0367, -0.0173,  0.0718],\n",
            "         [ 0.6812, -0.0315, -0.0236,  0.0646]]], grad_fn=<ViewBackward0>)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Block\n",
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "   def __init__(self,embed_dim):\n",
        "       super().__init__()\n",
        "       self.attn = SelfAttention(embed_dim)\n",
        "       self.ff = nn.Sequential(\n",
        "           nn.Linear(embed_dim,4*embed_dim),\n",
        "           nn.ReLU(),\n",
        "           nn.Linear(4*embed_dim,embed_dim)\n",
        "       )\n",
        "       self.norm1 = nn.LayerNorm(embed_dim)\n",
        "       self.norm2 = nn.LayerNorm(embed_dim)\n",
        "   def forward(self,x):\n",
        "       x = x + self.attn(self.norm1(x))\n",
        "       x = x + self.ff(self.norm2(x))\n",
        "       return x\n",
        "\n"
      ],
      "metadata": {
        "id": "ebG3t1VQyZw8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mini GPT\n",
        "\n",
        "class MiniGPT(nn.Module):\n",
        "   def __init__(self,vocab_size,embed_dim,num_layer):\n",
        "       super().__init__()\n",
        "       self.token_emb = nn.Embedding(vocab_size,embed_dim)\n",
        "       self.position_emb = nn.Embedding(100,embed_dim)\n",
        "       self.blocks = nn.Sequential(*[TransformerBlock(embed_dim) for _ in range(num_layer)])\n",
        "       self.ln = nn.LayerNorm(embed_dim)\n",
        "       self.fc = nn.Linear(embed_dim,vocab_size)\n",
        "\n",
        "   def forward(self, idx):\n",
        "     B, T = idx.shape\n",
        "     pos = torch.arange(T, device=idx.device).unsqueeze(0).expand(B, T)\n",
        "     x = self.token_emb(idx) + self.position_emb(pos)\n",
        "     x = self.blocks(x)\n",
        "     x = self.ln(x)\n",
        "     return self.fc(x)\n"
      ],
      "metadata": {
        "id": "SjF0scOFz-XJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Settings\n",
        "vocab_size = 1000\n",
        "embed_dim = 64\n",
        "num_layers = 2\n",
        "seq_len = 5\n",
        "batch_size = 2\n",
        "\n",
        "# Create model\n",
        "model = MiniGPT(vocab_size, embed_dim, num_layers)\n",
        "\n",
        "# Fake token input (batch of 2 sequences, each 5 tokens long)\n",
        "x = torch.randint(0, vocab_size, (batch_size, seq_len))  # shape [2, 5]\n",
        "print(\"Input token IDs:\\n\", x)\n",
        "\n",
        "# Forward pass\n",
        "output = model(x)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n",
        "# print(\"Sample output for first token in sequence:\\n\", output[0, 0])  # logits\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NC7eWgMK45N1",
        "outputId": "10d60538-440f-4f12-9ef3-55384fd01264"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input token IDs:\n",
            " tensor([[833, 573, 840, 246, 480],\n",
            "        [730, 754, 823, 708, 230]])\n",
            "Input shape: torch.Size([2, 5, 64])\n",
            "Input x:\n",
           
